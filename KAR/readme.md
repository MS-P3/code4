# Table of Contents

- [Table of Contents](#table-of-contents)
- [KAR Description](#kar-description)
- [Model Architecture](#model-architecture)
- [Dataset](#dataset)
- [Environment Requirements](#environment-requirements)
- [Quick Start](#quick-start)
- [Script Description](#script-description)
    - [Script and Sample Code](#script-and-sample-code)
        - [Script Parameters](#script-parameters)
- [Model Description](#model-description)
    - [Performance](#performance)
        - [Evaluation Performance](#evaluation-performance)
- [Description of Random Situation](#desciption-of-random-situation)
- [ModelZoo Homepage](#modelzoo-homepage)

# KAR Description

KAR is a model-agnostic framework that bridges classical recommender systems and open-world
knowledge, leveraging both reasoning and factual knowledge from
the LLMs. By first leveraging LLMs to generate open-world knowledge, and then applying classical recommender systems to model
the collaborative signals, we combine the advantages of both LLMs
and RSs and significantly improve the model’s predictive accuracy. See the paper [Towards Open-World Recommendation with Knowledge
Augmentation from Large Language Models
](https://arxiv.org/abs/2306.10933) for more details.

# Model Architecture

KAR is a model-agnostic framework, consisting of knowledge reasoning and generation stage, knowledge adaptation stage, and knowledge utilization stage.

- **Knowledge reasoning and generation stage** leverages our designed factorization prompting to extract the reasoning and factual knowledge from LLMs.
- **Knowledge adaptation stage** converts textual open-world knowledge into compact and the reasoning and fact augmented representations suitable for recommendation.
- **Knowledge utilization stage** integrates the reasoning and fact augmented vectors into an existing recommendation model.

# Dataset

- [MovieLens 1M Dataset](https://grouplens.org/datasets/movielens/1m/)

# Environment Requirements

- Framework
    - [MindSpore](https://gitee.com/mindspore/mindspore).  For more information, please check the resources below:
        - [MindSpore Tutorials](https://www.mindspore.cn/tutorials/zh-CN/master/index.html)

# Quick Start

1. Clone the code

```bash
git clone https://github.com/mindspore-lab/models.git
cd models/research/huawei-noah/KAR/src
```

2. Start training

The model can be trained and evaluated by the command as follows:

```bash
# Python command
python main_ctr.py
```

# Script Description

## Script and Data

```markdown
└── PAR
    ├── README.md                                 # Instruction and tutorial
    └── src                                       # dir for code
        ├── main_ctr.py                           # Python script of running the experiments
        ├── dataset.py                            # Create the dataset
        ├── utils.py                              # Other functions used in the data processing or model training
        └── models.py                             # KAR model
```

Here, we provide the knowledge generated by LLM which is available [Here](https://www.dropbox.com/scl/fi/1xm6p5bxnysjt0ffdz0oq/data.rar?rlkey=7orv241nkj4ld0ybwybtt7ihf&dl=0).

   1. `data/amz/knowledge/item.klg`: factual knowledge about books in Amazon-Books. Note that some books are filtered.
   2. `data/amz/knowledge/user.klg`: preference knowledge about users in Amazon-Books. Note that some users are filtered.
   3. `data/ml-1m/knowledge/item.klg`: factual knowledge about movies in MovieLens-1M.
   4. `data/ml-1m/knowledge/user.klg`: preference knowledge about users in MovieLens-1M.

Above files are all saved in `json`, with each key as the raw id of user/item in the original dataset and each value as the corresponding prompt and knowledge generated by LLM.

To avoid information leakage, we also provide `end_index.json`, where each user has an index n. We sort the interaction sequence of each user by time (in ascending order) and give each interaction an index. Then the first n interactions are used for prompting LLM to get preference knowledge. In our implementation, the first n interaction will not appear in the test set to prevent information leakage.

## Script Parameters

```markdown

Used by: main_ctr.py

Arguments:

  --data_dir                   train dir
  --setting_dir                dir for setting
  --save dir                   dir for saving
  --seed                       seed (Default: 1234)
  --epoch_num                  epochs of iteration (Default:20)
  --batch_size                 batch size (Default:256)
  --max_hist_len               the max length of history (Default:10)
  --lr                         learning rate(Default:1e-3)
  --weight_decay               weight decay (Default:0)
  --dropout                    dropout rate (Default:0.0)
  --convert_dropout            dropout rate for convert module (Default:0.0)
  --patience                   patience when early stop (Default:3)
  --augment                    whether to use augment vectors
  --aug_prefix                 prefix of augment file
  --embed_dim                  size of embedding (Default:16)
  --din_mlp                    MLP layer in DIN attention (Default: 32,16)
  --final_mlp_arch             size of final layer (Default:200,80)
  --convert_arch               size of convert net (MLP/expert net in HEA) Default: 128,32
  --expert_num                 number of shared expert IN HEA
  --specific_expert_num        the number of specific expert in HEA
```

## Description of Random Situation

- Shuffle of the dataset.
- Random seeds.
- Initialization of some model weights.
- Dropout operations.

# ModelZoo Homepage

Please check the official [homepage](https://github.com/mindspore-lab/models/tree/master).
